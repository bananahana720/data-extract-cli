"""Semantic pipeline orchestration for process jobs."""

from __future__ import annotations

import json
import time
from dataclasses import dataclass, field
from pathlib import Path
from typing import Any, Dict, Iterable, List

from data_extract.core.models import Chunk
from data_extract.semantic.lsa import LsaConfig, LsaReductionStage
from data_extract.semantic.quality_metrics import QualityConfig, QualityMetricsStage
from data_extract.semantic.reporting import (
    export_similarity_graph,
    generate_csv_report,
    generate_html_report,
)
from data_extract.semantic.similarity import SimilarityAnalysisStage, SimilarityConfig
from data_extract.semantic.tfidf import TfidfVectorizationStage
from data_extract.services.chunk_io import load_chunks
from data_extract.services.run_config_resolver import ResolvedSemanticConfig


@dataclass(frozen=True)
class SemanticArtifactRecord:
    """Filesystem artifact generated by semantic pipeline."""

    name: str
    path: Path
    artifact_type: str
    format: str


@dataclass
class SemanticRunResult:
    """Semantic runtime result used by job orchestration."""

    status: str = "disabled"
    message: str | None = None
    summary: Dict[str, Any] = field(default_factory=dict)
    artifacts: List[SemanticArtifactRecord] = field(default_factory=list)
    stage_timings_ms: Dict[str, float] = field(default_factory=dict)


class SemanticOrchestrationService:
    """Run semantic stages for process output chunks and persist artifacts."""

    def run(
        self,
        output_paths: Iterable[Path],
        artifact_root: Path,
        config: ResolvedSemanticConfig,
    ) -> SemanticRunResult:
        if not config.enabled:
            return SemanticRunResult(status="disabled", message="Semantic stage not requested.")

        chunks = self._load_output_chunks(output_paths)
        if not chunks:
            return SemanticRunResult(status="skipped", message="No semantic-ready chunks found.")

        semantic_dir = artifact_root / "semantic"
        semantic_dir.mkdir(parents=True, exist_ok=True)

        try:
            tfidf_start = time.perf_counter()
            tfidf_stage = TfidfVectorizationStage(
                config=self._build_tfidf_config(config),
            )
            tfidf_result = tfidf_stage.process(chunks, None)
            tfidf_ms = (time.perf_counter() - tfidf_start) * 1000
            if not tfidf_result.success:
                return SemanticRunResult(
                    status="failed",
                    message=tfidf_result.error or "TF-IDF stage failed.",
                    stage_timings_ms={"semantic_tfidf": tfidf_ms},
                )

            similarity_start = time.perf_counter()
            similarity_stage = SimilarityAnalysisStage(
                config=SimilarityConfig(
                    duplicate_threshold=config.duplicate_threshold,
                    related_threshold=config.related_threshold,
                    compute_graph=True,
                    use_cache=True,
                )
            )
            similarity_result = similarity_stage.process(tfidf_result, None)
            similarity_ms = (time.perf_counter() - similarity_start) * 1000
            if not similarity_result.success:
                return SemanticRunResult(
                    status="failed",
                    message=similarity_result.error or "Similarity stage failed.",
                    stage_timings_ms={
                        "semantic_tfidf": tfidf_ms,
                        "semantic_similarity": similarity_ms,
                    },
                )

            lsa_start = time.perf_counter()
            lsa_stage = LsaReductionStage(
                config=LsaConfig(
                    n_components=config.n_components,
                    use_cache=True,
                )
            )
            lsa_result = lsa_stage.process(similarity_result, None)
            lsa_ms = (time.perf_counter() - lsa_start) * 1000
            if not lsa_result.success:
                return SemanticRunResult(
                    status="failed",
                    message=lsa_result.error or "LSA stage failed.",
                    stage_timings_ms={
                        "semantic_tfidf": tfidf_ms,
                        "semantic_similarity": similarity_ms,
                        "semantic_lsa": lsa_ms,
                    },
                )

            quality_start = time.perf_counter()
            quality_stage = QualityMetricsStage(
                config=QualityConfig(
                    min_quality=config.min_quality,
                    use_cache=True,
                )
            )
            enriched_chunks = quality_stage.process(chunks, None)
            quality_ms = (time.perf_counter() - quality_start) * 1000

            results = _compile_results(lsa_result, enriched_chunks, config)

            artifacts: List[SemanticArtifactRecord] = []

            summary_path = semantic_dir / "semantic_summary.json"
            summary_path.write_text(json.dumps(results, indent=2, default=str), encoding="utf-8")
            artifacts.append(
                SemanticArtifactRecord(
                    name="Semantic Summary",
                    path=summary_path,
                    artifact_type="summary",
                    format="json",
                )
            )

            if config.report:
                report_artifact = self._write_report(
                    semantic_dir=semantic_dir,
                    report_format=config.report_format,
                    results=results,
                )
                if report_artifact is not None:
                    artifacts.append(report_artifact)

            if config.export_graph:
                graph_content = export_similarity_graph(results, output_format=config.graph_format)
                graph_path = semantic_dir / f"similarity_graph.{config.graph_format}"
                graph_path.write_text(graph_content, encoding="utf-8")
                artifacts.append(
                    SemanticArtifactRecord(
                        name="Similarity Graph",
                        path=graph_path,
                        artifact_type="graph",
                        format=config.graph_format,
                    )
                )

            stage_timings = {
                "semantic_tfidf": tfidf_ms,
                "semantic_similarity": similarity_ms,
                "semantic_lsa": lsa_ms,
                "semantic_quality": quality_ms,
            }
            return SemanticRunResult(
                status="completed",
                summary=results.get("summary", {}),
                artifacts=artifacts,
                stage_timings_ms=stage_timings,
            )
        except Exception as exc:
            return SemanticRunResult(status="failed", message=str(exc))

    @staticmethod
    def _build_tfidf_config(config: ResolvedSemanticConfig) -> Any:
        from data_extract.semantic.models import TfidfConfig

        return TfidfConfig(
            max_features=config.max_features,
            min_df=1,
            max_df=1.0,
            quality_threshold=0.0,
            use_cache=True,
        )

    @staticmethod
    def _load_output_chunks(output_paths: Iterable[Path]) -> List[Chunk]:
        chunks: List[Chunk] = []
        seen_ids: set[str] = set()
        for output_path in sorted(output_paths, key=lambda path: str(path)):
            for chunk in load_chunks(output_path):
                chunk_id = str(chunk.id)
                if chunk_id in seen_ids:
                    continue
                seen_ids.add(chunk_id)
                chunks.append(chunk)
        return chunks

    @staticmethod
    def _write_report(
        semantic_dir: Path,
        report_format: str,
        results: Dict[str, Any],
    ) -> SemanticArtifactRecord | None:
        report_format = report_format.lower()
        if report_format == "html":
            content = generate_html_report(results)
            report_path = semantic_dir / "semantic_report.html"
        elif report_format == "csv":
            content = generate_csv_report(results)
            report_path = semantic_dir / "semantic_report.csv"
        elif report_format == "json":
            content = json.dumps(results, indent=2, default=str)
            report_path = semantic_dir / "semantic_report.json"
        else:
            return None

        report_path.write_text(content, encoding="utf-8")
        return SemanticArtifactRecord(
            name="Semantic Report",
            path=report_path,
            artifact_type="report",
            format=report_format,
        )


def _compile_results(
    semantic_result: Any,
    enriched_chunks: List[Chunk],
    config: ResolvedSemanticConfig,
) -> Dict[str, Any]:
    data = semantic_result.data or {}
    metadata = semantic_result.metadata or {}

    return {
        "summary": {
            "total_chunks": len(enriched_chunks),
            "vocabulary_size": metadata.get("vocabulary_size", 0),
            "n_components": metadata.get("n_components", 0),
            "n_clusters": metadata.get("n_clusters", 0),
            "cache_hit": semantic_result.cache_hit,
            "processing_time_ms": semantic_result.processing_time_ms,
        },
        "similarity": {
            "duplicate_groups": data.get("duplicate_groups", []),
            "similar_pairs": data.get("similar_pairs", []),
            "similarity_graph": data.get("similarity_graph", {}),
            "n_duplicates": len(data.get("duplicate_groups", [])),
            "statistics": data.get("similarity_statistics", {}),
        },
        "topics": data.get("topics", {}),
        "clusters": {
            "assignments": (
                [int(cluster) for cluster in data.get("clusters", [])]
                if data.get("clusters") is not None
                else []
            ),
            "silhouette_score": data.get("silhouette_score", 0.0),
        },
        "quality": {
            "distribution": _quality_distribution(enriched_chunks),
            "mean_score": (
                sum(chunk.quality_score for chunk in enriched_chunks) / len(enriched_chunks)
                if enriched_chunks
                else 0.0
            ),
        },
        "config": {
            "tfidf_max_features": config.max_features,
            "similarity_threshold": config.duplicate_threshold,
            "related_threshold": config.related_threshold,
            "lsa_n_components": config.n_components,
            "quality_min_score": config.min_quality,
        },
    }


def _quality_distribution(chunks: List[Chunk]) -> Dict[str, int]:
    distribution = {"high": 0, "medium": 0, "low": 0, "review": 0}
    for chunk in chunks:
        score = float(getattr(chunk, "quality_score", 0.0))
        if score >= 0.7:
            distribution["high"] += 1
        elif score >= 0.3:
            distribution["medium"] += 1
        else:
            distribution["low"] += 1
    return distribution
