[pytest]
# Pytest configuration for data extraction tool

# Test discovery patterns
python_files = test_*.py
python_classes = Test*
python_functions = test_*

# Directories to search for tests
testpaths = tests
pythonpath = src

# Patterns to ignore during test collection
norecursedirs = .git .tox dist build *.egg __pycache__ **/TRASH **/.trash

# Additional command line options applied by default
addopts =
    # Verbose output showing test names
    -v
    # Show summary of all test outcomes
    -ra
    # Show local variables in tracebacks
    --showlocals
    # Strict markers - fail if unknown marker used
    --strict-markers
    # Fail on warnings (comment out during development if too strict)
    # --strict-warnings

# Minimum Python version
minversion = 3.11

# Custom markers for selective test execution
markers =
    unit: Unit tests (fast, isolated)
    integration: Integration tests (slower, multiple components)
    slow: Tests that may take more than 1 second
    performance: Performance and benchmarking tests
    extraction: Tests for extractor modules
    processing: Tests for processor modules
    formatting: Tests for formatter modules
    pipeline: Tests for pipeline orchestration
    cli: Command-line interface tests
    subprocess: CLI tests using subprocess (validates installed package)
    edge_case: Edge case and boundary condition tests
    stress: Resource-intensive stress tests
    infrastructure: Infrastructure component tests (config, logging, errors)
    cross_format: Cross-format validation and consistency tests
    chunking: Chunking module tests (Epic 3)
    entity_aware: Entity-aware chunking tests (Story 3.2)
    quality: Quality scoring tests (Story 3.3)
    output: Output formatting tests (Story 3.4)
    schema: JSON schema validation tests (Story 3.4)
    compatibility: Cross-platform compatibility tests (Story 3.4)
    organization: Output organization tests (Story 3.5)
    semantic: Semantic analysis tests (Epic 4)
    tfidf: TF-IDF vectorization tests
    similarity: Document similarity tests
    lsa: Latent Semantic Analysis tests
    quality_metrics: Text quality scoring tests
    epic4: Epic 4 specific tests
    epic5: Epic 5 specific tests (CLI UX & Batch Processing)
    story_4_2: Story 4-2 tests (Similarity Analysis)
    story_4_4: Story 4-4 tests (Quality Filtering)
    story_5_1: Story 5-1 tests (Refactored Command Structure)
    story_5_2: Story 5-2 tests (Configuration Management)
    story_5_3: Story 5-3 tests (Progress Indicators)
    story_5_6: Story 5-6 tests (Error Recovery)
    story_5_4: Story 5-4 tests (Summary Statistics)
    story_5_5: Story 5-5 tests (Preset Configurations)
    story_5_7: Story 5-7 tests (Batch Processing & Incremental)
    behavioral: Behavioral tests (GIVEN-WHEN-THEN pattern)
    uat: User Acceptance Tests
    journey: User journey tests
    e2e: End-to-end scenario identifier (provide ID string)
    p0: Critical-risk scenarios (run every PR)
    p1: High-priority regression scenarios
    p2: Medium-priority regression scenarios
    P0: Critical path tests - always run (CI/CD priority level)
    P1: Core functionality tests - run on PR (CI/CD priority level)
    P2: Extended coverage tests - run nightly (CI/CD priority level)
    test_id(id): Unique test identifier for traceability (e.g., "5.1-UNIT-001")

# Coverage options (using pytest-cov)
[coverage:run]
source = src
omit =
    */tests/*
    */test_*
    */__pycache__/*
    */venv/*
    */.venv/*

[coverage:report]
precision = 2
show_missing = True
skip_covered = False
# Fail if coverage is below 60% (Epic 1 baseline target)
fail_under = 60

[coverage:html]
directory = htmlcov

# Console output format
console_output_style = progress

# Logging
log_cli = false
log_cli_level = INFO
log_cli_format = %(asctime)s [%(levelname)8s] %(message)s
log_cli_date_format = %Y-%m-%d %H:%M:%S

# Ignore specific warnings
filterwarnings =
    # Ignore specific deprecation warnings if needed
    # ignore::DeprecationWarning

# Timeout for tests (requires pytest-timeout)
# Uncomment when pytest-timeout is installed:
# timeout = 300
# timeout_method = thread
